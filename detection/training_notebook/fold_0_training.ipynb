{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "instance_type": "ml.g4dn.xlarge",
    "kernelspec": {
      "display_name": "Python 3 (Data Science)",
      "language": "python",
      "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "fold 0 training.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxOAR2HgocNl"
      },
      "source": [
        "# ***YOLO V5 Installation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymWxeg3poXtk",
        "outputId": "dd179ff5-2ebc-40f3-cc64-d9126643e0f3"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git > /dev/null\n",
        "!cd yolov5 && git reset --hard  cce7e78\n",
        "!cd yolov5 && pip install -r requirements.txt > /dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 9827, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 9827 (delta 0), reused 2 (delta 0), pack-reused 9822\u001b[K\n",
            "Receiving objects: 100% (9827/9827), 10.15 MiB | 29.70 MiB/s, done.\n",
            "Resolving deltas: 100% (6822/6822), done.\n",
            "Checking out files: 100% (104/104), done.\n",
            "HEAD is now at cce7e78 Created using Colaboratory\n",
            "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
            "  from cryptography.utils import int_from_bytes\n",
            "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
            "  from cryptography.utils import int_from_bytes\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPVUlRjPoww-"
      },
      "source": [
        "# ***Helpers***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B1EBLhIoXts"
      },
      "source": [
        "import os\n",
        "import yaml\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "from shutil import copyfile\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from shutil import copyfile\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "def makedirs(path_to_save_labels, path_to_images, dataset_type2image):\n",
        "    for split_type in dataset_type2image.keys():\n",
        "        if not os.path.exists(os.path.join(path_to_save_labels, split_type)):\n",
        "            os.makedirs(os.path.join(path_to_save_labels, split_type))\n",
        "        if not os.path.exists(os.path.join(path_to_images, split_type)):\n",
        "            os.makedirs(os.path.join(path_to_images, split_type))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ConvertCOCOToYOLO:\n",
        "\n",
        "    \"\"\"\n",
        "    Takes in the path to COCO annotations and outputs YOLO annotations in multiple .txt files.\n",
        "    COCO annotation are to be JSON formart as follows:\n",
        "        \"annotations\":{\n",
        "            \"area\":2304645,\n",
        "            \"id\":1,\n",
        "            \"image_id\":10,\n",
        "            \"category_id\":4,\n",
        "            \"bbox\":[\n",
        "                0::704\n",
        "                1:620\n",
        "                2:1401\n",
        "                3:1645\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_folder, json_path,PATH_TO_SAVE_LABELS):\n",
        "        self.img_folder = img_folder\n",
        "        self.json_path = json_path\n",
        "        self.PATH_TO_SAVE_LABELS = PATH_TO_SAVE_LABELS\n",
        "\n",
        "    def get_img_shape(self, img_path):\n",
        "        img = cv2.imread(img_path)\n",
        "        try:\n",
        "            return img.shape\n",
        "        except AttributeError:\n",
        "            print('error!', img_path)\n",
        "            return (None, None, None)\n",
        "\n",
        "    def convert_labels(self, img_path, x1, y1, x2, y2):\n",
        "        \"\"\"\n",
        "        Definition: Parses label files to extract label and bounding box\n",
        "        coordinates. Converts (x1, y1, x1, y2) KITTI format to\n",
        "        (x, y, width, height) normalized YOLO format.\n",
        "        \"\"\"\n",
        "\n",
        "        def sorting(l1, l2):\n",
        "            if l1 > l2:\n",
        "                lmax, lmin = l1, l2\n",
        "                return lmax, lmin\n",
        "            else:\n",
        "                lmax, lmin = l2, l1\n",
        "                return lmax, lmin\n",
        "        size = (1080,1920)\n",
        "        xmax, xmin = sorting(x1, x2)\n",
        "        ymax, ymin = sorting(y1, y2)\n",
        "        dw = 1./size[1]\n",
        "        dh = 1./size[0]\n",
        "        x = (xmin + xmax)/2.0\n",
        "        y = (ymin + ymax)/2.0\n",
        "        w = xmax - xmin\n",
        "        h = ymax - ymin\n",
        "        x = x*dw\n",
        "        w = w*dw\n",
        "        y = y*dh\n",
        "        h = h*dh\n",
        "        return (x,y,w,h)\n",
        "\n",
        "    def convert(self,annotation_key='annotations',img_id='image_id',cat_id='category_id',bbox='bbox',image2dataset_type = None):\n",
        "        # Enter directory to read JSON file\n",
        "        data = json.load(open(self.json_path))\n",
        "        \n",
        "        check_set = set()\n",
        "\n",
        "        # Retrieve data\n",
        "        for i in range(len(data[annotation_key])):\n",
        "\n",
        "            # Get required data\n",
        "            image_id = data['annotations'][i]['image_id']\n",
        "            category_id = int(f'{data[annotation_key][i][cat_id]}') - 1\n",
        "            bbox = data[annotation_key][i]['bbox'] \n",
        "            # Retrieve image.\n",
        "            if self.img_folder == None:\n",
        "                image_path = f'{image_id}'\n",
        "            else:\n",
        "                image_path = f'./{self.img_folder}{image_id}'\n",
        "\n",
        "\n",
        "            # Convert the data\n",
        "            kitti_bbox = [bbox[0], bbox[1], bbox[2] + bbox[0], bbox[3] + bbox[1]]\n",
        "            yolo_bbox = self.convert_labels(image_path, kitti_bbox[0], kitti_bbox[1], kitti_bbox[2], kitti_bbox[3])\n",
        "            \n",
        "            # Prepare for export\n",
        "            \n",
        "            filename = f'{image_id}.txt'\n",
        "            content =f\"{category_id} {yolo_bbox[0]} {yolo_bbox[1]} {yolo_bbox[2]} {yolo_bbox[3]}\"\n",
        "            split_type = image2dataset_type[image_id]\n",
        "            # Export \n",
        "            if image_id in check_set:\n",
        "                # Append to existing file as there can be more than one label in each image\n",
        "                file = open(PATH_TO_SAVE_LABELS +split_type+'/'+ filename, \"a\")\n",
        "                file.write(\"\\n\")\n",
        "                file.write(content)\n",
        "                file.close()\n",
        "\n",
        "            elif image_id not in check_set:\n",
        "                check_set.add(image_id)\n",
        "                # Write files\n",
        "                file = open(PATH_TO_SAVE_LABELS +split_type+'/'+ filename, \"w\")\n",
        "                file.write(content)\n",
        "                file.close()\n",
        "            \n",
        "def get_yolo_labels(\n",
        "    path_to_bboxes, path_to_save_labels, path_to_images, image2dataset_type, class_names\n",
        "):\n",
        "    for file_name in os.listdir(path_to_bboxes):\n",
        "        with open(os.path.join(path_to_bboxes, file_name), \"r\") as f:\n",
        "            img_boxes = json.load(f)\n",
        "        img_name = file_name[:-5]\n",
        "        im = Image.open(os.path.join(path_to_images, img_name))\n",
        "        im_width, im_height = im.size\n",
        "        split_type = image2dataset_type[img_name]\n",
        "        if len(img_boxes[\"bb_objects\"]) > 0:\n",
        "            for box in img_boxes[\"bb_objects\"]:\n",
        "                x_center = ((box[\"x1\"] + box[\"x2\"]) / 2) / im_width\n",
        "                y_center = ((box[\"y1\"] + box[\"y2\"]) / 2) / im_height\n",
        "                width = (box[\"x2\"] - box[\"x1\"]) / im_width\n",
        "                height = (box[\"y2\"] - box[\"y1\"]) / im_height\n",
        "                label_class = class_names.index(box[\"class\"])\n",
        "                with open(\n",
        "                    os.path.join(\n",
        "                        path_to_save_labels, split_type, img_name[:-4] + \".txt\"\n",
        "                    ),\n",
        "                    \"a+\",\n",
        "                ) as f:\n",
        "                    f.write(f\"{label_class} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "        # –ï—Å–ª–∏ –Ω–µ —Ö–æ—Ç–∏—Ç–µ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å —Å–µ–º–ø–ª—ã –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏\n",
        "        else:\n",
        "            open(\n",
        "                os.path.join(path_to_save_labels, split_type, img_name[:-4] + \".txt\"),\n",
        "                \"a\",\n",
        "            ).close()\n",
        "\n",
        "\n",
        "def copy_images(image_dic,images_name, image2dataset_type, path_to_images):\n",
        "    for img_path in tqdm(images_name):\n",
        "        split_type = image2dataset_type[image_dic[img_path]]\n",
        "        try:\n",
        "            copyfile(\n",
        "                os.path.join(path_to_images, img_path),\n",
        "                os.path.join(path_to_images, split_type, (str(image_dic[img_path]) + '.jpg')),\n",
        "            )\n",
        "        except IsADirectoryError:\n",
        "            continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxQ7c5Uwo3Jl"
      },
      "source": [
        "# ***Prepare Data and YOLO Annotations***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JyEp7_HoXtx"
      },
      "source": [
        "PATH_TO_BBOXES = 'data_task2/train/annotations/COCO_json/'\n",
        "PATH_TO_IMAGES = 'data_task2/train/images/'\n",
        "PATH_TO_SAVE_LABELS = 'data_task2/train/labels/'\n",
        "\n",
        "CLASS_NAMES = ['Human']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EipCxLdoXtx",
        "outputId": "9eaa56a0-25de-4d14-dd0a-24dad3e5a93f"
      },
      "source": [
        "random.seed(0)\n",
        "IMAGES_NAME = os.listdir(PATH_TO_IMAGES)\n",
        "random.shuffle(IMAGES_NAME)\n",
        "train_images = IMAGES_NAME[:int(len(IMAGES_NAME)*0.8)]\n",
        "val_images = IMAGES_NAME[int(len(IMAGES_NAME)*0.8):]\n",
        "test_images = IMAGES_NAME[int(len(IMAGES_NAME)*0.8):]\n",
        "len(train_images), len(val_images), len(test_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(400, 100, 100)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIB9izSIoXty"
      },
      "source": [
        "dic = json.load(open(PATH_TO_BBOXES + 'coco_annotations_train.json'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE29WuKdoXty"
      },
      "source": [
        "image_dic = dict()\n",
        "for i in range(len(dic['images'])):\n",
        "    image_dic[dic['images'][i]['file_name']] = dic['images'][i]['id']\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNOOedKWoXtz"
      },
      "source": [
        "DATASET_TYPE2IMAGE = {'train': train_images,\n",
        "                      'val': val_images,\n",
        "                      'test': test_images}\n",
        "\n",
        "IMAGE2DATASET_TYPE = {}\n",
        "for key, values in DATASET_TYPE2IMAGE.items():\n",
        "    for file_name in values:\n",
        "        IMAGE2DATASET_TYPE[image_dic[file_name]] = key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHIcoW68oXtz"
      },
      "source": [
        "evra_dataset = [\"train: ../data_task2/train/images/train/\" + \"\\n\",\n",
        "                 \"val: ../data_task2/train/images/test/\" + \"\\n\",\n",
        "                 \"test: ../data_task2/train/images/test/\" + \"\\n\\n\",\n",
        "                 \"nc: 1\" + \"\\n\\n\",\n",
        "                 \"names: [ 'Human']\",\n",
        "                ]\n",
        "\n",
        "with open(r'yolov5/data/evra_dataset.yaml', 'w') as f:\n",
        "    f.writelines(evra_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqBtV7EsoXtz"
      },
      "source": [
        "makedirs(PATH_TO_SAVE_LABELS, PATH_TO_IMAGES, DATASET_TYPE2IMAGE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-kPkXG6oXt0"
      },
      "source": [
        "ConvertCOCOToYOLO(img_folder=PATH_TO_IMAGES,json_path=PATH_TO_BBOXES + 'coco_annotations_train.json',PATH_TO_SAVE_LABELS = PATH_TO_SAVE_LABELS).convert(image2dataset_type =IMAGE2DATASET_TYPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqEXccbgoXt0",
        "outputId": "d7ffc971-afc8-402c-f56f-d916970c73af"
      },
      "source": [
        "copy_images(image_dic,IMAGES_NAME, IMAGE2DATASET_TYPE, PATH_TO_IMAGES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:13<00:00, 35.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebaVhJhJoXt1"
      },
      "source": [
        "\n",
        "with open(\"yolov5/data/hyps/hyp.scratch.yaml\", \"r\") as f:\n",
        "    hyps = yaml.safe_load(f)\n",
        "    \n",
        "\n",
        "with open(\"yolov5/data/hyps/hyp_evra_.yaml\", 'w') as f:\n",
        "    yaml.dump(hyps, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgMSOjGYpAml"
      },
      "source": [
        "# ***Training***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "SaOJ3jY9oXt1",
        "outputId": "2135574b-e196-4320-e7cb-e65e338c3bf7"
      },
      "source": [
        "!cd yolov5 && python train.py --img 1280 --batch 8 --epochs 30 --data evra_dataset.yaml --weights yolov5m6.pt --hyp data/hyps/hyp_evra_.yaml --name exp6 --workers 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5m6.pt, cfg=, data=evra_dataset.yaml, hyp=data/hyps/hyp_evra_.yaml, epochs=30, batch_size=8, imgsz=1280, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=0, entity=None, project=runs/train, name=exp6, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0, patience=100\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 58 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
            "YOLOv5 üöÄ v5.0-491-gcce7e78 torch 1.10.0+cu102 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0manchor_t=4.0, box=0.05, cls=0.5, cls_pw=1.0, copy_paste=0.0, degrees=0.0, fl_gamma=0.0, fliplr=0.5, flipud=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, iou_t=0.2, lr0=0.01, lrf=0.2, mixup=0.0, momentum=0.937, mosaic=1.0, obj=1.0, obj_pw=1.0, perspective=0.0, scale=0.5, shear=0.0, translate=0.1, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 üöÄ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   1991808  models.common.Conv                      [384, 576, 3, 2]              \n",
            "  8                -1  2   2327040  models.common.C3                        [576, 576, 2]                 \n",
            "  9                -1  1   3982848  models.common.Conv                      [576, 768, 3, 2]              \n",
            " 10                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
            " 11                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
            " 12                -1  1    443520  models.common.Conv                      [768, 576, 1, 1]              \n",
            " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
            " 15                -1  2   2658816  models.common.C3                        [1152, 576, 2, False]         \n",
            " 16                -1  1    221952  models.common.Conv                      [576, 384, 1, 1]              \n",
            " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 20                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 24                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
            " 26                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 27                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
            " 29                -1  2   2437632  models.common.C3                        [768, 576, 2, False]          \n",
            " 30                -1  1   2987136  models.common.Conv                      [576, 576, 3, 2]              \n",
            " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
            " 32                -1  2   4429824  models.common.C3                        [1152, 768, 2, False]         \n",
            " 33  [23, 26, 29, 32]  1     34632  models.yolo.Detect                      [1, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [192, 384, 576, 768]]\n",
            "/opt/conda/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 481 layers, 35275944 parameters, 35275944 gradients, 49.1 GFLOPs\n",
            "\n",
            "Transferred 620/628 items from yolov5m6.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 103 weight, 107 weight (no decay), 107 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../data_task2/train/labels/train.cache' images and labels... 40\u001b[0m\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../data_task2/train/labels/test.cache' images and labels... 100 f\u001b[0m\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 6.09, Best Possible Recall (BPR) = 1.0000\n",
            "Image sizes 1280 train, 1280 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp66\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/29     11.9G   0.09004   0.06274         0        19      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184    0.00817      0.163    0.00392    0.00109\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/29     12.6G   0.06007   0.04832         0        36      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.102      0.195     0.0465     0.0148\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/29     12.6G   0.05005    0.0388         0        25      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.554      0.842      0.672      0.257\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/29     12.6G   0.04872    0.0257         0        31      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.846      0.815      0.895      0.471\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/29     12.6G   0.04968   0.02145         0        29      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.658      0.652      0.718      0.235\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/29     12.6G   0.05057   0.01897         0        38      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.952      0.868      0.961      0.568\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      6/29     12.6G   0.04627   0.01736         0        21      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.726      0.891       0.86      0.434\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      7/29     12.6G   0.04845   0.01651         0        20      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.872      0.777      0.884      0.316\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      8/29     12.6G   0.04631   0.01651         0        22      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.606      0.864      0.699      0.364\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      9/29     12.6G   0.04296   0.01481         0        17      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.866      0.924       0.94      0.477\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     10/29     12.6G   0.04436   0.01474         0        22      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.703      0.777      0.778        0.4\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     11/29     12.6G   0.04179   0.01401         0        20      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.725      0.945      0.861      0.331\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     12/29     12.6G   0.03878   0.01318         0        34      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.788      0.902      0.896      0.418\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     13/29     12.6G   0.04141   0.01278         0        13      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.842      0.902      0.907      0.479\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     14/29     12.6G    0.0382   0.01254         0        18      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.702      0.804      0.809      0.337\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     15/29     12.6G    0.0401   0.01224         0        20      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.721      0.897      0.868      0.412\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     16/29     12.6G   0.03981   0.01174         0        21      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.854      0.923      0.943      0.601\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     17/29     12.6G   0.03527   0.01191         0        29      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.921      0.902      0.966      0.636\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     18/29     12.6G   0.03259   0.01177         0        32      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.892      0.951      0.924      0.553\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     19/29     12.6G   0.03374   0.01123         0        30      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.977      0.946      0.975       0.52\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     20/29     12.6G   0.03013   0.01133         0        22      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.972      0.956      0.972      0.601\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     21/29     12.6G   0.02683   0.01103         0        34      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.972      0.935      0.976      0.646\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     22/29     12.6G    0.0255   0.01072         0        30      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.983      0.962      0.985      0.632\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     23/29     12.6G   0.02585    0.0108         0        21      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.972      0.962      0.983      0.641\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     24/29     12.6G   0.02379   0.01076         0        22      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.962      0.956      0.978      0.666\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     25/29     12.6G   0.02371  0.009762         0        33      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.967      0.951      0.968      0.658\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     26/29     12.6G   0.01989  0.009792         0        28      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.983       0.94      0.964      0.676\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     27/29     12.6G   0.02075  0.009705         0        22      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.967      0.951      0.968      0.703\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     28/29     12.6G   0.01997  0.009537         0        26      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.956      0.951       0.97      0.713\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     29/29     12.6G    0.0201  0.009572         0        25      1280: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.956      0.951      0.973       0.72\n",
            "\n",
            "30 epochs completed in 0.943 hours.\n",
            "Optimizer stripped from runs/train/exp66/weights/last.pt, 71.1MB\n",
            "Optimizer stripped from runs/train/exp66/weights/best.pt, 71.1MB\n",
            "\n",
            "Validating runs/train/exp66/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 378 layers, 35248920 parameters, 0 gradients, 49.0 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.956      0.951      0.973       0.72\n",
            "Results saved to \u001b[1mruns/train/exp66\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdN6mb_KpIBd"
      },
      "source": [
        "# ***Finetune for bigger image size***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DpT29pjoXt1"
      },
      "source": [
        "\n",
        "with open(\"yolov5/data/hyps/hyp_evra_.yaml\", \"r\") as f:\n",
        "    hyps = yaml.safe_load(f)\n",
        "    hyps['lr0'] = 0.0005\n",
        "    hyps['lrf'] = 1\n",
        "with open(\"yolov5/data/hyps/hyp_evra_.yaml\", 'w') as f:\n",
        "    yaml.dump(hyps, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E3etAvooXt1",
        "outputId": "e8bbb9fd-168a-4173-c3a0-f7624038dd42"
      },
      "source": [
        "!cd yolov5 && python train.py --img 2048 --batch 2 --epochs 10 --data evra_dataset.yaml --weights runs/train/exp66/weights/best.pt --hyp data/hyps/hyp_evra_.yaml --name exp2048 --workers 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=runs/train/exp66/weights/best.pt, cfg=, data=evra_dataset.yaml, hyp=data/hyps/hyp_evra_.yaml, epochs=10, batch_size=2, imgsz=2048, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=0, entity=None, project=runs/train, name=exp2048, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0, patience=100\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 58 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
            "YOLOv5 üöÄ v5.0-491-gcce7e78 torch 1.10.0+cu102 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0manchor_t=4.0, box=0.05, cls=0.5, cls_pw=1.0, copy_paste=0.0, degrees=0.0, fl_gamma=0.0, fliplr=0.5, flipud=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, iou_t=0.2, lr0=0.0005, lrf=1, mixup=0.0, momentum=0.937, mosaic=1.0, obj=1.0, obj_pw=1.0, perspective=0.0, scale=0.5, shear=0.0, translate=0.1, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 üöÄ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   1991808  models.common.Conv                      [384, 576, 3, 2]              \n",
            "  8                -1  2   2327040  models.common.C3                        [576, 576, 2]                 \n",
            "  9                -1  1   3982848  models.common.Conv                      [576, 768, 3, 2]              \n",
            " 10                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
            " 11                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
            " 12                -1  1    443520  models.common.Conv                      [768, 576, 1, 1]              \n",
            " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
            " 15                -1  2   2658816  models.common.C3                        [1152, 576, 2, False]         \n",
            " 16                -1  1    221952  models.common.Conv                      [576, 384, 1, 1]              \n",
            " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 20                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 24                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
            " 26                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 27                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
            " 29                -1  2   2437632  models.common.C3                        [768, 576, 2, False]          \n",
            " 30                -1  1   2987136  models.common.Conv                      [576, 576, 3, 2]              \n",
            " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
            " 32                -1  2   4429824  models.common.C3                        [1152, 768, 2, False]         \n",
            " 33  [23, 26, 29, 32]  1     34632  models.yolo.Detect                      [1, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [192, 384, 576, 768]]\n",
            "/opt/conda/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 481 layers, 35275944 parameters, 35275944 gradients, 49.1 GFLOPs\n",
            "\n",
            "Transferred 628/628 items from runs/train/exp66/weights/best.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 103 weight, 107 weight (no decay), 107 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../data_task2/train/labels/train.cache' images and labels... 40\u001b[0m\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../data_task2/train/labels/test.cache' images and labels... 100 f\u001b[0m\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 6.47, Best Possible Recall (BPR) = 1.0000\n",
            "Image sizes 2048 train, 2048 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2048\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/9     7.73G   0.01961    0.0131         0         1      2048: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.962      0.956      0.978      0.693\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/9     8.17G   0.01926   0.01333         0        12      2048: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.967      0.951      0.974        0.7\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/9     8.17G   0.01831    0.0127         0        11      2048: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.967      0.951      0.973      0.701\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/9     8.17G    0.0177   0.01241         0         5      2048: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.957      0.956      0.974      0.704\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/9     8.17G   0.01718   0.01217         0        12      2048: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.956      0.957      0.979      0.707\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/9     8.17G   0.01714   0.01205         0        14      2048: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.962      0.951      0.977      0.708\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/9     8.17G   0.01684   0.01185         0         5      2048: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.967      0.946      0.977      0.716\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       7/9     8.17G   0.01675   0.01196         0         5      2048: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.967      0.951      0.978      0.719\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       8/9     8.17G   0.01609   0.01246         0        13      2048: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.967      0.951      0.978      0.721\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       9/9     8.17G   0.01561    0.0115         0         5      2048: 100%|‚ñà| \n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.951      0.951      0.972      0.719\n",
            "\n",
            "10 epochs completed in 0.608 hours.\n",
            "Optimizer stripped from runs/train/exp2048/weights/last.pt, 71.3MB\n",
            "Optimizer stripped from runs/train/exp2048/weights/best.pt, 71.3MB\n",
            "\n",
            "Validating runs/train/exp2048/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 378 layers, 35248920 parameters, 0 gradients, 49.0 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
            "                 all        100        184      0.967      0.951      0.978      0.721\n",
            "Results saved to \u001b[1mruns/train/exp2048\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}